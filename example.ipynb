{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import models, layers\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, BatchNormalization, Conv3D\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training': {'window_size': 21, 'batch_size': 32, 'patience': 15, 'epsilon': 1e-06, 'random_state': 42, 'epochs': 100, 'covariates': 'None'}, 'model': {'run': 'long_ttm', 'filters': 2, 'kernel_size': [2, 2], 'strides': 1, 'kernel_initializer': 'glorot_uniform', 'recurrent_initializer': 'orthogonal', 'optimizer': 'adam', 'lr': 0.001, 'covariates': ['VIX', 'VVIX', 'SKEW']}, 'forecast': {'h_step': 1}}\n"
     ]
    }
   ],
   "source": [
    "with open('configs/config_file.yaml') as file:\n",
    "    params = yaml.safe_load(file)\n",
    "\n",
    "print(params)\n",
    "\n",
    "window_size = params['training']['window_size']\n",
    "h_step = params['forecast']['h_step']\n",
    "patience = params['training']['patience']\n",
    "epsilon = params['training']['epsilon']\n",
    "batch_size = params['training']['batch_size']\n",
    "epochs = params['training']['epochs']\n",
    "\n",
    "run = params['model']['run']\n",
    "learning_rate = params['model']['lr']\n",
    "covariate_columns = params['model']['covariates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's reshape our input data of the thing... we are going to need labels, and we are going to need train surface.\n",
    "# The labels will be, the smoothed IVs of our data\n",
    "# The train will be the dimensions, with time x ttm x moneyness encoders\n",
    "# If we have covariates, the channels will be larger? -> Yes, starting channels will be added to the layers\n",
    "\n",
    "# Load the data first\n",
    "if run == 'short_ttm':\n",
    "    data_train = pd.read_csv('data/final/smoothed/data_train.csv')\n",
    "    data_val = pd.read_csv('data/final/evaluation/validation_set.csv')\n",
    "    data_test = pd.read_csv('data/final/evaluation/test_set.csv')\n",
    "\n",
    "    if covariate_columns is not None:\n",
    "        covar_df = pd.read_excel('data/final/covariates/covariates_train.xlsx')\n",
    "        covar_df_val = pd.read_excel('data/final/covariates/covariates_validation.xlsx')\n",
    "\n",
    "        covar_df = covar_df.rename(columns={'Date':'date'})\n",
    "        covar_df_val = covar_df_val.rename(columns={'Date':'date'})\n",
    "        covar_df = covar_df[['date'] + covariate_columns]\n",
    "        covar_df_val = covar_df_val[['date'] + covariate_columns]\n",
    "\n",
    "elif run == 'long_ttm':\n",
    "    data_train = pd.read_csv('data/final/smoothed/data_train_long.csv')\n",
    "    data_val = pd.read_csv('data/final/evaluation/validation_set_long.csv')\n",
    "    data_test = pd.read_csv('data/final/evaluation/test_set_long.csv')\n",
    "\n",
    "    if covariate_columns is not None:\n",
    "        covar_df = pd.read_excel('data/final/covariates/covariates_train_long.xlsx')\n",
    "        covar_df_val = pd.read_excel('data/final/covariates/covariates_validation_long.xlsx')\n",
    "\n",
    "        covar_df = covar_df.rename(columns={'Date':'date'})\n",
    "        covar_df_val = covar_df_val.rename(columns={'Date':'date'})\n",
    "        covar_df = covar_df[['date'] + covariate_columns]\n",
    "        covar_df_val = covar_df_val[['date'] + covariate_columns]\n",
    "   \n",
    "else:\n",
    "    print('Select a dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['date'] = pd.to_datetime(data_train['date'])\n",
    "data_val['date'] = pd.to_datetime(data_val['date'])\n",
    "data_test['date'] = pd.to_datetime(data_test['date'])\n",
    "data_train = pd.merge(data_train, covar_df, on='date', how='left')\n",
    "data_val = pd.merge(data_val, covar_df, on='date', how='left')\n",
    "data_test = pd.merge(data_test, covar_df, on='date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5673528, 32)\n",
      "(5517184, 32)\n",
      "(300251, 31)\n",
      "(294479, 31)\n",
      "(1618161, 31)\n",
      "(1553959, 31)\n"
     ]
    }
   ],
   "source": [
    "def process(data):\n",
    "    data = data.drop(columns=\"Unnamed: 0\")\n",
    "\n",
    "    # Let's implement the thing, where deep OTM, OTM, ATM, ITM, deep ITM is a thing\n",
    "\n",
    "    # we have to discriminate between calls and puts\n",
    "    # Coding; deep OTM = 1, OTM =2, ATM =3, ITM = 4, deep ITM=5 \n",
    "    # outliers, sort of?\n",
    "    print(data.shape)\n",
    "    data = data[data['moneyness'] >= 0.8]\n",
    "    data = data[data['moneyness'] <= 1.6]\n",
    "    print(data.shape)\n",
    "\n",
    "    # Also consider what to do with low volume... probably include them and acknowledge them as a limitation\n",
    "\n",
    "    data.loc[(data['cp_flag']=='C') & (data['moneyness'] <0.90), 'moneyness_enc'] = 1\n",
    "    data.loc[(data['cp_flag']=='C') & (data['moneyness'] >=0.90) & (data['moneyness'] < 0.97), 'moneyness_enc'] = 2\n",
    "    data.loc[(data['cp_flag']=='C') & (data['moneyness'] >=0.97) & (data['moneyness'] < 1.03), 'moneyness_enc'] = 3\n",
    "    data.loc[(data['cp_flag']=='C') & (data['moneyness'] >=1.03) & (data['moneyness'] < 1.10), 'moneyness_enc'] = 4\n",
    "    data.loc[(data['cp_flag']=='C') & (data['moneyness'] >=1.10), 'moneyness_enc'] = 5\n",
    "\n",
    "    data.loc[(data['cp_flag']=='P') & (data['moneyness'] <0.90), 'moneyness_enc'] = 5\n",
    "    data.loc[(data['cp_flag']=='P') & (data['moneyness'] >=0.90) & (data['moneyness'] < 0.97), 'moneyness_enc'] = 4\n",
    "    data.loc[(data['cp_flag']=='P') & (data['moneyness'] >=0.97) & (data['moneyness'] < 1.03), 'moneyness_enc'] = 3\n",
    "    data.loc[(data['cp_flag']=='P') & (data['moneyness'] >=1.03) & (data['moneyness'] < 1.10), 'moneyness_enc'] = 2\n",
    "    data.loc[(data['cp_flag']=='P') & (data['moneyness'] >=1.10), 'moneyness_enc'] = 1\n",
    "    return data\n",
    "\n",
    "data_train = process(data_train)\n",
    "data_val = process(data_val)\n",
    "data_test = process(data_test)\n",
    "\n",
    "# Thing to fix: The moneyness encoded, results in multiple impl volatility values for the same moneyness, maturity\n",
    "# combination. To fix this, take the average, and omit the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def frame_to_numpy(data, eval=False):\n",
    "#     # Convert 'time_step' to datetime\n",
    "#     data['time_step'] = pd.to_datetime(data['date'])\n",
    "\n",
    "#     # Create a time_step index (e.g., from the first unique date)\n",
    "#     time_step_index = pd.to_datetime(data['time_step']).dt.strftime('%Y-%m-%d').unique()\n",
    "\n",
    "#     # Map time_step dates to integer index\n",
    "#     data['time_step_idx'] = data['time_step'].apply(lambda x: np.where(time_step_index == x.strftime('%Y-%m-%d'))[0][0])\n",
    "#     print(data['time_step_idx'])\n",
    "\n",
    "#     maturity_values = np.sort(data['maturity'].unique())\n",
    "#     maturity_to_idx = {mat: i for i, mat in enumerate(maturity_values)}\n",
    "\n",
    "#     time_steps = len(time_step_index)\n",
    "#     money_dim = len(data['moneyness_enc'].unique())\n",
    "#     ttm_dim = len(data['maturity'].unique())\n",
    "\n",
    "#     # Create an empty numpy array with the shape (time_steps, height_dim, width_dim)\n",
    "#     IV_array = np.zeros((time_steps, money_dim, ttm_dim, ), dtype=np.float32)\n",
    "\n",
    "#     # Populate the numpy array with values from the DataFrame\n",
    "#     for idx, row in data.iterrows():\n",
    "#         time_step_idx = row['time_step_idx']\n",
    "#         height = int(row['moneyness_enc']) - 1 \n",
    "#         width = maturity_to_idx[row['maturity']]\n",
    "        \n",
    "#         if eval==False:\n",
    "#             value = row['IV_smooth']\n",
    "#         else:\n",
    "#             value = row['impl_volatility']\n",
    "            \n",
    "#         # print(time_step_idx, height, width, value)\n",
    "#         # Assign the value to the corresponding position in the numpy array\n",
    "#         IV_array[time_step_idx, height, width] = value\n",
    "        \n",
    "#     IV_array = IV_array.reshape((IV_array.shape[0], money_dim, ttm_dim, 1))\n",
    "#     return IV_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_to_numpy(data, covariate_cols=None, eval=False):\n",
    "    \n",
    "    data['time_step'] = pd.to_datetime(data['date'])\n",
    "    time_step_index = pd.to_datetime(data['time_step']).dt.strftime('%Y-%m-%d').unique()\n",
    "    data['time_step_idx'] = data['time_step'].apply(lambda x: np.where(time_step_index == x.strftime('%Y-%m-%d'))[0][0])\n",
    "\n",
    "    maturity_values = np.sort(data['maturity'].unique())\n",
    "    maturity_to_idx = {mat: i for i, mat in enumerate(maturity_values)}\n",
    "\n",
    "    time_steps = len(time_step_index)\n",
    "    money_dim = len(data['moneyness_enc'].unique())\n",
    "    ttm_dim = len(maturity_values)\n",
    "\n",
    "    # Base IV tensor\n",
    "    IV_array = np.zeros((time_steps, money_dim, ttm_dim))\n",
    "\n",
    "    # If covariates provided, create tensor to hold them\n",
    "    covariate_arrays = {}\n",
    "    if covariate_cols:\n",
    "        for cov in covariate_cols:\n",
    "            covariate_arrays[cov] = np.zeros((time_steps, money_dim, ttm_dim), dtype=np.float32)\n",
    "\n",
    "    for idx, row in data.iterrows():\n",
    "        time_step_idx = row['time_step_idx']\n",
    "        height = int(row['moneyness_enc']) - 1 \n",
    "        width = maturity_to_idx[row['maturity']]\n",
    "        value = row['IV_smooth'] if not eval else row['impl_volatility']\n",
    "        IV_array[time_step_idx, height, width] = value\n",
    "\n",
    "        # Also fill in covariates\n",
    "        if covariate_cols:\n",
    "            for cov in covariate_cols:\n",
    "                covariate_arrays[cov][time_step_idx, height, width] = row[cov]\n",
    "\n",
    "    # Reshape and concatenate\n",
    "    IV_array = IV_array.reshape((time_steps, money_dim, ttm_dim, 1))\n",
    "\n",
    "    if covariate_cols:\n",
    "        covariate_stack = [arr.reshape((time_steps, money_dim, ttm_dim, 1)) for arr in covariate_arrays.values()]\n",
    "        covariate_stack = np.concatenate(covariate_stack, axis=-1)  # shape: (T, H, W, C)\n",
    "        IV_array = np.concatenate([IV_array, covariate_stack], axis=-1)  # final shape: (T, H, W, 1+C)\n",
    "\n",
    "    return IV_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IV_train = frame_to_numpy(data_train, covariate_columns)\n",
    "IV_val = frame_to_numpy(data_val, covariate_columns, eval=True)\n",
    "IV_test = frame_to_numpy(data_test, covariate_columns, eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IV_train = frame_to_numpy(data_train)\n",
    "# IV_val = frame_to_numpy(data_val, eval=True)\n",
    "# IV_test = frame_to_numpy(data_test, eval=True)\n",
    "# 7 min on pc, for long set\n",
    "# Write array to data folder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2416, 5, 260, 4) (218, 5, 260, 4) (253, 5, 260, 4)\n"
     ]
    }
   ],
   "source": [
    "print(IV_train.shape, IV_val.shape, IV_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert 'time_step' to datetime\n",
    "# data_train['time_step'] = pd.to_datetime(data_train['date'])\n",
    "\n",
    "# # Create a time_step index (e.g., from the first unique date)\n",
    "# time_step_index = pd.to_datetime(data_train['time_step']).dt.strftime('%Y-%m-%d').unique()\n",
    "\n",
    "# # Map time_step dates to integer index\n",
    "# data_train['time_step_idx'] = data_train['time_step'].apply(lambda x: np.where(time_step_index == x.strftime('%Y-%m-%d'))[0][0])\n",
    "# print(data_train['time_step_idx'])\n",
    "# ttm_dim = 5\n",
    "# money_dim = 5\n",
    "# time_steps = len(time_step_index)\n",
    "\n",
    "# # Create an empty numpy array with the shape (time_steps, height_dim, width_dim)\n",
    "# IV_array = np.zeros((time_steps, ttm_dim, money_dim))\n",
    "\n",
    "# # Populate the numpy array with values from the DataFrame\n",
    "# for idx, row in data_train.iterrows():\n",
    "#     time_step_idx = row['time_step_idx']\n",
    "#     width = row['maturity'] - 1 \n",
    "#     height = int(row['moneyness_enc']) - 1 \n",
    "#     value = row['IV_smooth']\n",
    "#     print(time_step_idx, width, height, value)\n",
    "#     # Assign the value to the corresponding position in the numpy array\n",
    "#     IV_array[time_step_idx, height, width] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(IV_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IV_array = IV_array.reshape((IV_array.shape[0], 5, 5, 1))\n",
    "# print(IV_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2415, 5, 260, 4)\n"
     ]
    }
   ],
   "source": [
    "print(IV_train[:-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2395, 5, 260, 1)\n"
     ]
    }
   ],
   "source": [
    "print(IV_train[window_size:][:,:,:,0:1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    data=IV_train,\n",
    "    targets=IV_train[window_size:][:,:,:,0:1], # Select only the IV, not the covariates\n",
    "    sequence_length=window_size,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=(TensorSpec(shape=(None, None, 5, 260, 4), dtype=tf.float64, name=None), TensorSpec(shape=(None, 5, 260, 1), dtype=tf.float64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window_size = 21 # about one month\n",
    "# labels = IV_array[1:]\n",
    "# X = IV_array[:-1]\n",
    "\n",
    "# dataset_train = tf.keras.utils.timeseries_dataset_from_array(\n",
    "#     data=IV_train[:-1],\n",
    "#     targets=IV_train[window_size:],\n",
    "#     sequence_length=window_size,\n",
    "#     batch_size=batch_size\n",
    "# )\n",
    "\n",
    "# Add the last timepoints of the dataset to the validation set, for the computation of the\n",
    "# validation set performance is calculated within the window size too\n",
    "# So the validation set should start from the end of the training set\n",
    "\n",
    "IV_val_input = np.concatenate((IV_train[-window_size:], IV_val), axis=0)\n",
    "\n",
    "dataset_val = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    data=IV_val_input,\n",
    "    targets=IV_val_input[window_size:][:,:,:,0:1],\n",
    "    sequence_length=window_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "IV_test_input = np.concatenate((IV_val[-window_size:], IV_test), axis=0)\n",
    "\n",
    "dataset_test= tf.keras.utils.timeseries_dataset_from_array(\n",
    "    data=IV_test_input,\n",
    "    targets=IV_test_input[window_size:][:,:,:,0:1],\n",
    "    sequence_length=window_size,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model(n_params, \n",
    "#                  dropout, \n",
    "#                  recurrent_dropout, \n",
    "#                  n_convlstm_layers = 2,\n",
    "#                  hidden_activation =  tf.keras.activations.tanh, \n",
    "#                  optimizer = keras.optimizers.Adam()):\n",
    "\n",
    "#     # input layer\n",
    "#     input_layer = layers.Input(shape= (None,5,5,1) )\n",
    "    \n",
    "#     # lstm layers\n",
    "#     lstm = input_layer\n",
    "#     for i in range( n_convlstm_layers ):\n",
    "#         lstm =  layers.ConvLSTM2D( \n",
    "#             kernel_size= (1,1), \n",
    "#             filters=n_params, \n",
    "#             data_format= 'channels_last', \n",
    "#             return_sequences = i<n_convlstm_layers-1,\n",
    "#             activation=hidden_activation,\n",
    "#             padding = \"same\",\n",
    "#             dropout=dropout, \n",
    "#             recurrent_dropout=recurrent_dropout\n",
    "#         )( lstm )\n",
    "#         lstm = layers.BatchNormalization()(lstm)    \n",
    "\n",
    "#     output = layers.Conv2D(\n",
    "#         filters=1, kernel_size=(1, 1), activation=\"linear\", padding=\"same\"\n",
    "#     )( lstm )\n",
    "#     output_layer = layers.Reshape((5,5))(output)\n",
    "\n",
    "#     # compile\n",
    "#     model = models.Model( input_layer, output_layer )\n",
    "#     model.compile(\n",
    "#         loss= \"MAE\",\n",
    "#         optimizer=optimizer, \n",
    "#     ) \n",
    "    \n",
    "#     print(model.summary())\n",
    "#     return model\n",
    "# model = create_model(n_params=10,dropout=0.1,recurrent_dropout=0.1,n_convlstm_layers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(model, \n",
    "#                 x_train, \n",
    "#                 y_train,\n",
    "#                 verbose = True, \n",
    "#                 save : \"dir\" = False,\n",
    "#                 training_kwarg_overwrites : \"dict\" = {} ):\n",
    "    \n",
    "#     # train until we run out of improvement\n",
    "#     callbacks = [\n",
    "#         keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),\n",
    "#         keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=15),\n",
    "#     ]\n",
    "    \n",
    "#     # train model\n",
    "#     training_kwargs = {\n",
    "#         \"x\" : x_train,\n",
    "#         \"y\" : y_train, #dataset[\"train\"][\"y_scaled\"],\n",
    "#         \"epochs\" : 200,\n",
    "#         \"batch_size\" : 64,\n",
    "#         \"verbose\" : verbose,\n",
    "#         \"validation_split\" : 0.2,\n",
    "#         \"callbacks\" : callbacks,\n",
    "#     } \n",
    "#     training_kwargs.update(training_kwarg_overwrites)\n",
    "#     train_hist = model.fit( **training_kwargs )\n",
    "    \n",
    "    \n",
    "#     if save:\n",
    "#         Path(save).mkdir(parents=True, exist_ok=True) # make a home for the models\n",
    "#         train_start, train_end = [ f( dataset[\"dates\"][\"train\"] ) for f in (min,max) ]\n",
    "#         model_name = \"-\".join( date.strftime(\"%Y%m%d\") for date in [train_start, train_end] )\n",
    "#         model.save( save+model_name )\n",
    "        \n",
    "#     return model, train_hist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import ConvLSTM2D, BatchNormalization, Flatten, Dense\n",
    "\n",
    "# time_steps = window_size\n",
    "# height = 5\n",
    "# width = 5\n",
    "# channels = 1\n",
    "\n",
    "# # Model definition\n",
    "# model = Sequential([\n",
    "#     ConvLSTM2D(filters=64, kernel_size=(3,3), activation='relu', \n",
    "#                return_sequences=True, input_shape=(time_steps, height, width, channels)),\n",
    "#     BatchNormalization(),\n",
    "#     ConvLSTM2D(filters=32, kernel_size=(3,3), activation='relu', return_sequences=False),\n",
    "#     Flatten(),\n",
    "#     Dense(128, activation='relu'),\n",
    "#     Dense(1)  # Predicting IV at a future time\n",
    "# ])\n",
    "\n",
    "# # Compile model\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv_lstm2d_4 (ConvLSTM2D)  (None, 21, 5, 260, 64)    156928    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 21, 5, 260, 64)   256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv_lstm2d_5 (ConvLSTM2D)  (None, 5, 260, 64)        295168    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 5, 260, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 5, 260, 1)         65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 452,673\n",
      "Trainable params: 452,417\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "TIME_STEPS = window_size\n",
    "HEIGHT = len(data_train['moneyness_enc'].unique())\n",
    "WIDTH = len(data_train['maturity'].unique())\n",
    "CHANNELS = 1 +len(covariate_columns)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# ConvLSTM2D expects 5D input: (batch, time, height, width, channels)\n",
    "model.add(ConvLSTM2D(filters=64, kernel_size=(3, 3),\n",
    "                     padding='same', return_sequences=True,\n",
    "                     input_shape=(TIME_STEPS, HEIGHT, WIDTH, CHANNELS)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(ConvLSTM2D(filters=64, kernel_size=(3, 3),\n",
    "                     padding='same', return_sequences=False))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Final 3D convolution to map to the next frame\n",
    "model.add(tf.keras.layers.Conv2D(filters=1, kernel_size=(1, 1),\n",
    "                                 activation='sigmoid', padding='same'))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon)\n",
    "model.compile(loss='mse', optimizer=optimizer)\n",
    "\n",
    "# Double check the architecture, and the activaiton function\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=(TensorSpec(shape=(None, None, 5, 260, 4), dtype=tf.float64, name=None), TensorSpec(shape=(None, 5, 260, 1), dtype=tf.float64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "print(dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "23/75 [========>.....................] - ETA: 12s - loss: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Takes about 40 min on dell xps laptop, 6.5 min on PC for short term\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 24 min on PC for long term\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Deepm\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Deepm\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Deepm\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Deepm\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Deepm\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Deepm\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Deepm\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Deepm\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Deepm\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(dataset_train, epochs=epochs, validation_data=dataset_val, callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                    patience=patience,\n",
    "                                                    mode='min')])\n",
    "\n",
    "# Takes about 40 min on dell xps laptop, 6.5 min on PC for short term\n",
    "# 24 min on PC for long term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 83ms/step\n",
      "8/8 [==============================] - 1s 82ms/step\n"
     ]
    }
   ],
   "source": [
    "# Next step; functions for the IVRMSE and R_oos!!!!!\n",
    "# H-step ahead performance!!!!!!\n",
    "# WRITE IT ALL TO RESULTS!!!!!!\n",
    "# Lower learning rate, beneficial, or leave it? Probably leave it\n",
    "# All the HYPERPARAMETERS!!!!!! kernel strides, window sizes, parameters, layers, ALL OF THEM\n",
    "# COVARIATES!!!!!!, Do all the hyperparameters again, and then only save the TEST PERFORMANCE\n",
    "# MODEL ARCHITECTURE!!!!\n",
    "# Investigate what happens, if you leave it as 0... We cannot do the interpolation properly, to be frank.\n",
    "\n",
    "#THEN LONG TERM !!!!!!\n",
    "# Transformer models!!!!!!!!!!!!!!!!\n",
    "pred_val = model.predict(dataset_val)\n",
    "pred_test = model.predict(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it in the compile, but also call it afterward\n",
    "# double check formula and also make it for R_oos\n",
    "# Should also work per h-step ahead \n",
    "def calculate_ivrmse(y_true, y_pred, all_points=False):\n",
    "    if not all_points:\n",
    "        ivrmse = tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "    else:\n",
    "        sq_error = tf.square(y_true - y_pred)\n",
    "        error_surface = tf.reduce_mean(sq_error, axis=[1 , 2])\n",
    "        ivrmse = tf.sqrt(error_surface)\n",
    "\n",
    "    return ivrmse.numpy()\n",
    "\n",
    "# model.compile(optimizer='adam', loss='mse', metrics=[ivrmse_metric])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(218, 5, 260, 1) (218, 5, 260, 1) (274, 5, 260, 1) (253, 5, 260, 1)\n"
     ]
    }
   ],
   "source": [
    "print(IV_val.shape, pred_val.shape, IV_test_input.shape, IV_test_input[:-window_size].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=(TensorSpec(shape=(None, None, 5, 260, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 5, 260, 1), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "print(dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape: (32, 21, 5, 260, 1)\n",
      "Target batch shape: (32, 5, 260, 1)\n"
     ]
    }
   ],
   "source": [
    "for x_batch, y_batch in dataset_val.take(1):\n",
    "    print(\"Input batch shape:\", x_batch.shape)\n",
    "    print(\"Target batch shape:\", y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.027347255\n",
      "0.027347255\n"
     ]
    }
   ],
   "source": [
    "print(calculate_ivrmse(IV_val, pred_val))\n",
    "print(calculate_ivrmse(IV_val, pred_val, all_points=False))\n",
    "# The loss is less, than in the optimization.. probably a different metric, or formula than mse\n",
    "# check values of the papers that are like yours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6825859\n"
     ]
    }
   ],
   "source": [
    "def calculate_r_oos(y_true, y_pred, all_points=False):\n",
    "    if not all_points:\n",
    "        ss_res = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "        mean_IV = tf.reduce_mean(y_true, axis=[1, 2], keepdims=True) # should be shape of 114 long\n",
    "        # print(mean_IV[0], mean_IV[1], mean_IV[2])\n",
    "        # print(y_true[0], y_true[1], y_true[2])\n",
    "        ss_tot = tf.reduce_sum(tf.square(y_true - mean_IV))\n",
    "        # print((y_true - mean_IV)[0])\n",
    "        r2 = 1 - ss_res/ss_tot\n",
    "    else:\n",
    "        ss_res = tf.reduce_sum(tf.square(y_true - y_pred), axis=[1, 2])\n",
    "        mean_IV = tf.reduce_mean(y_true, axis=[1, 2], keepdims=True)\n",
    "        ss_tot = tf.reduce_sum(tf.square(y_true - mean_IV), axis=[1, 2])\n",
    "        r2 = 1 - ss_res/ss_tot\n",
    "    return r2.numpy()\n",
    "\n",
    "print(calculate_r_oos(IV_val, pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model again on BOTH train and validation, and then investigate the TEST PERFORMANCE!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05364826\n",
      "0.70764846\n"
     ]
    }
   ],
   "source": [
    "print(calculate_ivrmse(IV_test, pred_test))\n",
    "print(calculate_r_oos(IV_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(y_real, y_pred):\n",
    "    ivrmse = calculate_ivrmse(y_real, y_pred)\n",
    "    ivrmse_h = calculate_ivrmse(y_real, y_pred, all_points=True)\n",
    "    r_oos = calculate_r_oos(y_real, y_pred)\n",
    "    r_oos_h = calculate_r_oos(y_real, y_pred, all_points=True)\n",
    "\n",
    "    return ivrmse, ivrmse_h, r_oos, r_oos_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(folder_path, ivrmse, r_oos, ivrmse_h, r_oos_h, surface, surface_pred):\n",
    "\n",
    "    ivrmse_path = folder_path / Path(\"ivrmse\")\n",
    "    r_oos_path = folder_path / Path(\"r_oos\")\n",
    "    ivrmse_h_path = folder_path / Path(\"ivrmse_h\")\n",
    "    r_oos_h_path = folder_path / Path(\"r_oos_h\")\n",
    "    surface_path = folder_path / Path(\"surface\")\n",
    "    surface_pred_path = folder_path / Path(\"surface_pred\")\n",
    "\n",
    "    if not ivrmse_path.exists():\n",
    "        ivrmse_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if not r_oos_path.exists():\n",
    "        r_oos_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if not ivrmse_h_path.exists():\n",
    "        ivrmse_h_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if not r_oos_h_path.exists():\n",
    "        r_oos_h_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if not surface_path.exists():\n",
    "        surface_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if not surface_pred_path.exists():\n",
    "        surface_pred_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    np.save(ivrmse_path / f\"{window_size}_{h_step}.npy\", ivrmse)\n",
    "    np.save(r_oos_path / f\"{window_size}_{h_step}.npy\", r_oos)\n",
    "    np.save(ivrmse_h_path / f\"{window_size}_{h_step}.npy\", ivrmse_h)\n",
    "    np.save(r_oos_h_path / f\"{window_size}_{h_step}.npy\", r_oos_h)\n",
    "    np.save(surface_path/ f\"{window_size}_{h_step}.npy\", surface)\n",
    "    np.save(surface_pred_path / f\"{window_size}_{h_step}.npy\", surface_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = Path(f\"results/test_{run}\")\n",
    "ivrmse, ivrmse_h, r_oos, r_oos_h = get_results(IV_test, pred_test)\n",
    "write_results(folder_path, ivrmse, r_oos, ivrmse_h, r_oos_h, IV_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.227813  ]\n",
      " [ 0.83448577]\n",
      " [ 0.82664824]\n",
      " [ 0.86329985]\n",
      " [ 0.82719207]\n",
      " [ 0.82161885]\n",
      " [ 0.859445  ]\n",
      " [ 0.8038464 ]\n",
      " [ 0.82923454]\n",
      " [ 0.8613956 ]\n",
      " [ 0.8831786 ]\n",
      " [ 0.8605362 ]\n",
      " [ 0.80928403]\n",
      " [ 0.7487365 ]\n",
      " [ 0.82359606]\n",
      " [ 0.8038391 ]\n",
      " [ 0.870738  ]\n",
      " [ 0.879892  ]\n",
      " [ 0.88754183]\n",
      " [ 0.715389  ]\n",
      " [ 0.8423885 ]\n",
      " [ 0.83931565]\n",
      " [ 0.71127   ]\n",
      " [ 0.7170203 ]\n",
      " [ 0.83064073]\n",
      " [ 0.88917625]\n",
      " [ 0.8844631 ]\n",
      " [ 0.8380521 ]\n",
      " [ 0.78856057]\n",
      " [ 0.7900923 ]\n",
      " [ 0.8124328 ]\n",
      " [ 0.81729823]\n",
      " [ 0.8404102 ]\n",
      " [ 0.802576  ]\n",
      " [ 0.83177733]\n",
      " [-0.4169848 ]\n",
      " [ 0.808445  ]\n",
      " [ 0.76371753]\n",
      " [ 0.6336098 ]\n",
      " [ 0.7836803 ]\n",
      " [ 0.84073335]\n",
      " [ 0.6562091 ]\n",
      " [ 0.8399776 ]\n",
      " [ 0.7637213 ]\n",
      " [ 0.92217815]\n",
      " [ 0.7660601 ]\n",
      " [ 0.7994304 ]\n",
      " [ 0.87237227]\n",
      " [ 0.7656763 ]\n",
      " [ 0.8554688 ]\n",
      " [ 0.7668767 ]\n",
      " [ 0.85255224]\n",
      " [ 0.76106083]\n",
      " [ 0.793423  ]\n",
      " [ 0.8994974 ]\n",
      " [ 0.7147583 ]\n",
      " [ 0.7361706 ]\n",
      " [ 0.67779636]\n",
      " [ 0.7745245 ]\n",
      " [ 0.761217  ]\n",
      " [ 0.76068795]\n",
      " [ 0.8049597 ]\n",
      " [ 0.7416584 ]\n",
      " [ 0.8276144 ]\n",
      " [ 0.7777232 ]\n",
      " [-0.09736204]\n",
      " [ 0.7291926 ]\n",
      " [ 0.7606807 ]\n",
      " [ 0.76246166]\n",
      " [ 0.67180836]\n",
      " [ 0.8053347 ]\n",
      " [ 0.72459066]\n",
      " [ 0.7481346 ]\n",
      " [ 0.76555234]\n",
      " [ 0.63931435]\n",
      " [ 0.8956195 ]\n",
      " [ 0.849574  ]\n",
      " [ 0.7871424 ]\n",
      " [ 0.73427117]\n",
      " [-0.08904445]\n",
      " [ 0.7736247 ]\n",
      " [ 0.8012702 ]\n",
      " [ 0.7484782 ]\n",
      " [ 0.5251986 ]\n",
      " [ 0.6867963 ]\n",
      " [ 0.82443947]\n",
      " [ 0.6891538 ]\n",
      " [ 0.74394786]\n",
      " [-0.0590539 ]\n",
      " [ 0.7357842 ]\n",
      " [ 0.8432713 ]\n",
      " [ 0.84843147]\n",
      " [ 0.5544919 ]\n",
      " [ 0.8105114 ]\n",
      " [ 0.7375479 ]\n",
      " [ 0.6944964 ]\n",
      " [ 0.7077718 ]\n",
      " [ 0.62093604]\n",
      " [ 0.6308421 ]\n",
      " [ 0.7417906 ]\n",
      " [ 0.68557346]\n",
      " [ 0.72794694]\n",
      " [ 0.6659372 ]\n",
      " [ 0.66497254]\n",
      " [ 0.6834824 ]\n",
      " [ 0.7589317 ]\n",
      " [ 0.69590795]\n",
      " [ 0.7050325 ]\n",
      " [ 0.7463095 ]\n",
      " [ 0.83144236]\n",
      " [ 0.6373528 ]\n",
      " [ 0.6940234 ]\n",
      " [ 0.80751973]\n",
      " [ 0.6216166 ]\n",
      " [ 0.5839952 ]\n",
      " [ 0.6685654 ]\n",
      " [ 0.7928947 ]\n",
      " [ 0.80135584]\n",
      " [ 0.7949435 ]\n",
      " [ 0.71254814]\n",
      " [ 0.69358814]\n",
      " [ 0.7271559 ]\n",
      " [ 0.7494731 ]\n",
      " [ 0.7657534 ]\n",
      " [ 0.69570357]\n",
      " [ 0.5649334 ]\n",
      " [ 0.6863359 ]\n",
      " [ 0.60104454]\n",
      " [ 0.8422128 ]\n",
      " [ 0.61967516]\n",
      " [ 0.76364934]\n",
      " [ 0.7541346 ]\n",
      " [-0.12772572]\n",
      " [ 0.7103153 ]\n",
      " [ 0.7119436 ]\n",
      " [ 0.72643495]\n",
      " [ 0.75123405]\n",
      " [ 0.4816643 ]\n",
      " [ 0.72714233]\n",
      " [ 0.7389842 ]\n",
      " [ 0.65733385]\n",
      " [ 0.6759623 ]\n",
      " [ 0.64074564]\n",
      " [ 0.6967167 ]\n",
      " [ 0.5237063 ]\n",
      " [ 0.7653815 ]\n",
      " [ 0.8468694 ]\n",
      " [ 0.71371967]\n",
      " [ 0.843791  ]\n",
      " [ 0.7612474 ]\n",
      " [ 0.77294886]\n",
      " [ 0.6928339 ]\n",
      " [ 0.8301225 ]\n",
      " [ 0.6524259 ]\n",
      " [ 0.8254216 ]\n",
      " [ 0.8137313 ]\n",
      " [ 0.8104731 ]\n",
      " [ 0.80421257]\n",
      " [ 0.74903584]\n",
      " [ 0.80799633]\n",
      " [ 0.7731738 ]\n",
      " [ 0.7262235 ]\n",
      " [ 0.77139324]\n",
      " [ 0.80671173]\n",
      " [ 0.7768762 ]\n",
      " [ 0.73706836]\n",
      " [ 0.61910987]\n",
      " [ 0.78721195]\n",
      " [ 0.73926747]\n",
      " [ 0.7103828 ]\n",
      " [ 0.77320623]\n",
      " [ 0.7499157 ]\n",
      " [ 0.7319994 ]\n",
      " [ 0.64460206]\n",
      " [ 0.7323885 ]\n",
      " [ 0.6929048 ]\n",
      " [ 0.741868  ]\n",
      " [ 0.7402605 ]\n",
      " [ 0.6989496 ]\n",
      " [ 0.6634227 ]\n",
      " [ 0.70784664]\n",
      " [ 0.6277746 ]\n",
      " [ 0.5686613 ]\n",
      " [ 0.6952229 ]\n",
      " [ 0.7464341 ]\n",
      " [ 0.7845924 ]\n",
      " [ 0.79223764]\n",
      " [ 0.79743695]\n",
      " [ 0.82270557]\n",
      " [-0.14883983]\n",
      " [ 0.70573497]\n",
      " [ 0.7718508 ]\n",
      " [ 0.73513925]\n",
      " [ 0.6316386 ]\n",
      " [ 0.7706529 ]\n",
      " [ 0.6844686 ]\n",
      " [ 0.8186956 ]\n",
      " [ 0.7370869 ]\n",
      " [ 0.81806135]\n",
      " [ 0.8661414 ]\n",
      " [ 0.6819769 ]\n",
      " [ 0.8347368 ]\n",
      " [ 0.7889293 ]\n",
      " [ 0.7530556 ]\n",
      " [ 0.7994151 ]\n",
      " [ 0.74899775]\n",
      " [ 0.83274204]\n",
      " [ 0.6406779 ]\n",
      " [ 0.7670976 ]\n",
      " [ 0.7746904 ]\n",
      " [ 0.00167322]\n",
      " [ 0.73630273]\n",
      " [ 0.837427  ]\n",
      " [ 0.7789593 ]\n",
      " [-0.15836203]\n",
      " [ 0.59663665]\n",
      " [ 0.6796603 ]\n",
      " [ 0.6843854 ]\n",
      " [ 0.75593066]\n",
      " [ 0.68715256]\n",
      " [ 0.73230076]\n",
      " [ 0.70891595]\n",
      " [ 0.76593864]\n",
      " [-0.02317262]\n",
      " [ 0.6771194 ]\n",
      " [ 0.66433024]\n",
      " [ 0.71414447]\n",
      " [ 0.57913345]\n",
      " [ 0.7387915 ]\n",
      " [ 0.71222615]\n",
      " [ 0.666886  ]\n",
      " [ 0.7925205 ]\n",
      " [ 0.56117475]\n",
      " [ 0.7398432 ]\n",
      " [ 0.71248347]\n",
      " [ 0.5687316 ]\n",
      " [ 0.76016   ]\n",
      " [ 0.63790137]\n",
      " [ 0.6515024 ]\n",
      " [ 0.72087   ]\n",
      " [ 0.7103609 ]\n",
      " [ 0.7583214 ]\n",
      " [ 0.6759307 ]\n",
      " [ 0.7216793 ]\n",
      " [ 0.62457204]\n",
      " [ 0.74527717]\n",
      " [ 0.73687136]\n",
      " [ 0.10072291]\n",
      " [ 0.63095224]\n",
      " [ 0.7421856 ]\n",
      " [ 0.7965652 ]\n",
      " [ 0.750754  ]\n",
      " [ 0.6699369 ]]\n"
     ]
    }
   ],
   "source": [
    "print(r_oos_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = Path(f\"results/validation_{run}\")\n",
    "ivrmse, ivrmse_h, r_oos, r_oos_h = get_results(IV_val, pred_val)\n",
    "write_results(folder_path, ivrmse, r_oos, ivrmse_h, r_oos_h, IV_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the h-step ahead forecast, an autoregressive approach is used\n",
    "# Predict the one step ahead forecast (done now)\n",
    "# After, use this one step ahead forecast in the predict command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253, 5, 260, 1)\n"
     ]
    }
   ],
   "source": [
    "print(pred_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
