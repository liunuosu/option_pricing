{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import models, layers\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, BatchNormalization, Conv3D\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training': {'window_size': 21, 'batch_size': 32, 'patience': 15, 'epsilon': 1e-06, 'random_state': 42, 'epochs': 100, 'covariates': 'None'}, 'model': {'run': 'long_ttm', 'filters': 2, 'kernel_size': [2, 2], 'strides': 1, 'kernel_initializer': 'glorot_uniform', 'recurrent_initializer': 'orthogonal', 'optimizer': 'adam', 'lr': 0.001, 'covariates': ['VIX', 'VVIX', 'SKEW', 'RVOL', 'TMS', 'CRS', 'EPU', 'ADS']}, 'forecast': {'h_step': 1}}\n"
     ]
    }
   ],
   "source": [
    "with open('configs/config_file.yaml') as file:\n",
    "    params = yaml.safe_load(file)\n",
    "\n",
    "print(params)\n",
    "\n",
    "window_size = params['training']['window_size']\n",
    "h_step = params['forecast']['h_step']\n",
    "patience = params['training']['patience']\n",
    "epsilon = params['training']['epsilon']\n",
    "batch_size = params['training']['batch_size']\n",
    "epochs = params['training']['epochs']\n",
    "\n",
    "run = params['model']['run']\n",
    "learning_rate = params['model']['lr']\n",
    "covariate_columns = params['model']['covariates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "covar_df = pd.read_excel('data/final/covariates_train.xlsx')\n",
    "covar_df = covar_df.rename(columns={'Date':'date'})\n",
    "# covars_val = pd.read_excel('data/final/covariates_validation.xlsx')\n",
    "covar_df = covar_df[['date'] + covariate_columns]\n",
    "# Difference between the two is, that the train file uses standardization calculated based on the train set\n",
    "# validation file standardizes using calculation based on train + validation set\n",
    "# Here, we need the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's reshape our input data of the thing... we are going to need labels, and we are going to need train surface.\n",
    "# The labels will be, the smoothed IVs of our data\n",
    "# The train will be the dimensions, with time x ttm x moneyness encoders\n",
    "# If we have covariates, the channels will be larger? -> Yes, starting channels will be added to the layers\n",
    "\n",
    "# Load the data first\n",
    "if run == 'short_ttm':\n",
    "    data_train = pd.read_csv('data/final/smoothed/data_train.csv')\n",
    "    data_val = pd.read_csv('data/final/evaluation/validation_set.csv')\n",
    "    data_test = pd.read_csv('data/final/evaluation/test_set.csv')\n",
    "elif run == 'long_ttm':\n",
    "    data_train = pd.read_csv('data/final/smoothed/data_train_long.csv')\n",
    "    data_val = pd.read_csv('data/final/evaluation/validation_set_long.csv')\n",
    "    data_test = pd.read_csv('data/final/evaluation/test_set_long.csv')\n",
    "else:\n",
    "    print('Select a dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the covariates to the columns of the datasets, based on date\n",
    "# covar_df['date'] = pd.to_datetime(covar_df['date'])\n",
    "data_train['date'] = pd.to_datetime(data_train['date'])\n",
    "data_train = pd.merge(data_train, covar_df, on='date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Timestamp('2014-07-17 00:00:00'), Timestamp('2014-11-20 00:00:00'), Timestamp('2013-04-12 00:00:00'), Timestamp('2013-01-11 00:00:00'), Timestamp('2013-03-14 00:00:00'), Timestamp('2015-06-18 00:00:00'), Timestamp('2015-01-09 00:00:00'), Timestamp('2012-11-15 00:00:00'), Timestamp('2012-08-10 00:00:00'), Timestamp('2013-10-17 00:00:00'), Timestamp('2013-11-14 00:00:00'), Timestamp('2013-12-19 00:00:00'), Timestamp('2015-11-19 00:00:00'), Timestamp('2014-12-12 00:00:00'), Timestamp('2014-06-19 00:00:00'), Timestamp('2014-06-13 00:00:00'), Timestamp('2015-04-02 00:00:00'), Timestamp('2013-05-16 00:00:00'), Timestamp('2015-07-16 00:00:00'), Timestamp('2013-01-17 00:00:00'), Timestamp('2015-07-02 00:00:00'), Timestamp('2012-10-12 00:00:00'), Timestamp('2013-09-19 00:00:00'), Timestamp('2012-09-14 00:00:00'), Timestamp('2012-01-19 00:00:00'), Timestamp('2012-12-20 00:00:00'), Timestamp('2012-07-13 00:00:00'), Timestamp('2013-10-11 00:00:00'), Timestamp('2013-07-12 00:00:00'), Timestamp('2012-05-17 00:00:00'), Timestamp('2012-02-16 00:00:00'), Timestamp('2012-12-14 00:00:00'), Timestamp('2015-12-11 00:00:00'), Timestamp('2014-09-12 00:00:00'), Timestamp('2012-06-14 00:00:00'), Timestamp('2012-01-13 00:00:00'), Timestamp('2012-03-15 00:00:00'), Timestamp('2016-01-14 00:00:00'), Timestamp('2014-08-08 00:00:00'), Timestamp('2013-03-28 00:00:00'), Timestamp('2012-06-08 00:00:00'), Timestamp('2014-02-14 00:00:00'), Timestamp('2012-08-16 00:00:00'), Timestamp('2016-02-18 00:00:00'), Timestamp('2015-03-19 00:00:00'), Timestamp('2014-10-10 00:00:00'), Timestamp('2014-01-16 00:00:00'), Timestamp('2012-04-19 00:00:00'), Timestamp('2012-02-10 00:00:00'), Timestamp('2013-06-14 00:00:00'), Timestamp('2014-05-15 00:00:00'), Timestamp('2015-01-15 00:00:00'), Timestamp('2012-07-19 00:00:00'), Timestamp('2014-03-14 00:00:00'), Timestamp('2014-08-14 00:00:00'), Timestamp('2013-08-15 00:00:00'), Timestamp('2015-10-15 00:00:00'), Timestamp('2013-12-13 00:00:00'), Timestamp('2014-05-09 00:00:00'), Timestamp('2014-07-03 00:00:00'), Timestamp('2013-06-20 00:00:00'), Timestamp('2014-04-17 00:00:00'), Timestamp('2014-03-20 00:00:00'), Timestamp('2014-04-11 00:00:00'), Timestamp('2012-05-11 00:00:00'), Timestamp('2012-09-20 00:00:00'), Timestamp('2014-02-20 00:00:00'), Timestamp('2014-07-11 00:00:00'), Timestamp('2012-04-05 00:00:00'), Timestamp('2015-08-20 00:00:00'), Timestamp('2013-11-08 00:00:00'), Timestamp('2014-10-16 00:00:00'), Timestamp('2014-01-10 00:00:00'), Timestamp('2013-02-14 00:00:00'), Timestamp('2013-08-09 00:00:00'), Timestamp('2013-02-08 00:00:00'), Timestamp('2013-09-13 00:00:00'), Timestamp('2012-04-13 00:00:00'), Timestamp('2013-07-18 00:00:00'), Timestamp('2013-03-08 00:00:00'), Timestamp('2012-10-18 00:00:00'), Timestamp('2013-05-10 00:00:00'), Timestamp('2015-09-17 00:00:00'), Timestamp('2015-04-16 00:00:00'), Timestamp('2014-09-18 00:00:00'), Timestamp('2015-12-31 00:00:00'), Timestamp('2012-03-09 00:00:00'), Timestamp('2015-02-19 00:00:00'), Timestamp('2015-05-14 00:00:00'), Timestamp('2014-11-14 00:00:00'), Timestamp('2014-12-18 00:00:00'), Timestamp('2013-04-18 00:00:00'), Timestamp('2012-11-09 00:00:00')}\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "print(set(data_train['date'])- set(covar_df['date']))\n",
    "print(len(set(data_train['date'])- set(covar_df['date'])))\n",
    "\n",
    "#Things to check; which days are missing, how we can include them, and if the covars are recorded in the morning or evening\n",
    "# Looks like 93 dates missing, but how can this be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VIX      88204\n",
       "VVIX     88938\n",
       "SKEW     97518\n",
       "RVOL     95239\n",
       "TMS     126410\n",
       "CRS      88204\n",
       "EPU      88204\n",
       "ADS      88204\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[covariate_columns].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIX      0\n",
      "VVIX     1\n",
      "SKEW     3\n",
      "RVOL     2\n",
      "TMS     21\n",
      "CRS      0\n",
      "EPU      0\n",
      "ADS      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(covar_df[covariate_columns].isna().sum()) # Not that many nans here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5673528, 29)\n",
      "(5517184, 29)\n",
      "(300251, 28)\n",
      "(294479, 28)\n",
      "(1618161, 28)\n",
      "(1553959, 28)\n"
     ]
    }
   ],
   "source": [
    "def process(data):\n",
    "    data = data.drop(columns=\"Unnamed: 0\")\n",
    "\n",
    "    # Let's implement the thing, where deep OTM, OTM, ATM, ITM, deep ITM is a thing\n",
    "\n",
    "    # we have to discriminate between calls and puts\n",
    "    # Coding; deep OTM = 1, OTM =2, ATM =3, ITM = 4, deep ITM=5 \n",
    "    # outliers, sort of?\n",
    "    print(data.shape)\n",
    "    data = data[data['moneyness'] >= 0.8]\n",
    "    data = data[data['moneyness'] <= 1.6]\n",
    "    print(data.shape)\n",
    "\n",
    "    # Also consider what to do with low volume... probably include them and acknowledge them as a limitation\n",
    "\n",
    "    data.loc[(data['cp_flag']=='C') & (data['moneyness'] <0.90), 'moneyness_enc'] = 1\n",
    "    data.loc[(data['cp_flag']=='C') & (data['moneyness'] >=0.90) & (data['moneyness'] < 0.97), 'moneyness_enc'] = 2\n",
    "    data.loc[(data['cp_flag']=='C') & (data['moneyness'] >=0.97) & (data['moneyness'] < 1.03), 'moneyness_enc'] = 3\n",
    "    data.loc[(data['cp_flag']=='C') & (data['moneyness'] >=1.03) & (data['moneyness'] < 1.10), 'moneyness_enc'] = 4\n",
    "    data.loc[(data['cp_flag']=='C') & (data['moneyness'] >=1.10), 'moneyness_enc'] = 5\n",
    "\n",
    "    data.loc[(data['cp_flag']=='P') & (data['moneyness'] <0.90), 'moneyness_enc'] = 5\n",
    "    data.loc[(data['cp_flag']=='P') & (data['moneyness'] >=0.90) & (data['moneyness'] < 0.97), 'moneyness_enc'] = 4\n",
    "    data.loc[(data['cp_flag']=='P') & (data['moneyness'] >=0.97) & (data['moneyness'] < 1.03), 'moneyness_enc'] = 3\n",
    "    data.loc[(data['cp_flag']=='P') & (data['moneyness'] >=1.03) & (data['moneyness'] < 1.10), 'moneyness_enc'] = 2\n",
    "    data.loc[(data['cp_flag']=='P') & (data['moneyness'] >=1.10), 'moneyness_enc'] = 1\n",
    "    return data\n",
    "\n",
    "data_train = process(data_train)\n",
    "data_val = process(data_val)\n",
    "data_test = process(data_test)\n",
    "\n",
    "# Thing to fix: The moneyness encoded, results in multiple impl volatility values for the same moneyness, maturity\n",
    "# combination. To fix this, take the average, and omit the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_to_numpy(data, eval=False):\n",
    "    # Convert 'time_step' to datetime\n",
    "    data['time_step'] = pd.to_datetime(data['date'])\n",
    "\n",
    "    # Create a time_step index (e.g., from the first unique date)\n",
    "    time_step_index = pd.to_datetime(data['time_step']).dt.strftime('%Y-%m-%d').unique()\n",
    "\n",
    "    # Map time_step dates to integer index\n",
    "    data['time_step_idx'] = data['time_step'].apply(lambda x: np.where(time_step_index == x.strftime('%Y-%m-%d'))[0][0])\n",
    "    print(data['time_step_idx'])\n",
    "\n",
    "    maturity_values = np.sort(data['maturity'].unique())\n",
    "    maturity_to_idx = {mat: i for i, mat in enumerate(maturity_values)}\n",
    "\n",
    "\n",
    "    time_steps = len(time_step_index)\n",
    "    money_dim = len(data['moneyness_enc'].unique())\n",
    "    ttm_dim = len(data['maturity'].unique())\n",
    "\n",
    "    # Create an empty numpy array with the shape (time_steps, height_dim, width_dim)\n",
    "    IV_array = np.zeros((time_steps, money_dim, ttm_dim, ), dtype=np.float32)\n",
    "\n",
    "    # Populate the numpy array with values from the DataFrame\n",
    "    for idx, row in data.iterrows():\n",
    "        time_step_idx = row['time_step_idx']\n",
    "        height = int(row['moneyness_enc']) - 1 \n",
    "        width = maturity_to_idx[row['maturity']]\n",
    "        \n",
    "        if eval==False:\n",
    "            value = row['IV_smooth']\n",
    "        else:\n",
    "            value = row['impl_volatility']\n",
    "            \n",
    "        # print(time_step_idx, height, width, value)\n",
    "        # Assign the value to the corresponding position in the numpy array\n",
    "        IV_array[time_step_idx, height, width] = value\n",
    "        \n",
    "    IV_array = IV_array.reshape((IV_array.shape[0], money_dim, ttm_dim, 1))\n",
    "    return IV_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0             0\n",
      "1             0\n",
      "2             0\n",
      "3             0\n",
      "4             0\n",
      "           ... \n",
      "5673523    2415\n",
      "5673524    2415\n",
      "5673525    2415\n",
      "5673526    2415\n",
      "5673527    2415\n",
      "Name: time_step_idx, Length: 5517184, dtype: int64\n",
      "0           0\n",
      "1           0\n",
      "2           0\n",
      "3           0\n",
      "4           0\n",
      "         ... \n",
      "300246    217\n",
      "300247    217\n",
      "300248    217\n",
      "300249    217\n",
      "300250    217\n",
      "Name: time_step_idx, Length: 294479, dtype: int64\n",
      "0            0\n",
      "1            0\n",
      "2            0\n",
      "3            0\n",
      "4            0\n",
      "          ... \n",
      "1618156    252\n",
      "1618157    252\n",
      "1618158    252\n",
      "1618159    252\n",
      "1618160    252\n",
      "Name: time_step_idx, Length: 1553959, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "IV_train = frame_to_numpy(data_train)\n",
    "IV_val = frame_to_numpy(data_val, eval=True)\n",
    "IV_test = frame_to_numpy(data_test, eval=True)\n",
    "# 7 min on pc, for long set\n",
    "# Write array to data folder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2416, 5, 260, 1) (218, 5, 260, 1) (253, 5, 260, 1)\n"
     ]
    }
   ],
   "source": [
    "print(IV_train.shape, IV_val.shape, IV_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert 'time_step' to datetime\n",
    "# data_train['time_step'] = pd.to_datetime(data_train['date'])\n",
    "\n",
    "# # Create a time_step index (e.g., from the first unique date)\n",
    "# time_step_index = pd.to_datetime(data_train['time_step']).dt.strftime('%Y-%m-%d').unique()\n",
    "\n",
    "# # Map time_step dates to integer index\n",
    "# data_train['time_step_idx'] = data_train['time_step'].apply(lambda x: np.where(time_step_index == x.strftime('%Y-%m-%d'))[0][0])\n",
    "# print(data_train['time_step_idx'])\n",
    "# ttm_dim = 5\n",
    "# money_dim = 5\n",
    "# time_steps = len(time_step_index)\n",
    "\n",
    "# # Create an empty numpy array with the shape (time_steps, height_dim, width_dim)\n",
    "# IV_array = np.zeros((time_steps, ttm_dim, money_dim))\n",
    "\n",
    "# # Populate the numpy array with values from the DataFrame\n",
    "# for idx, row in data_train.iterrows():\n",
    "#     time_step_idx = row['time_step_idx']\n",
    "#     width = row['maturity'] - 1 \n",
    "#     height = int(row['moneyness_enc']) - 1 \n",
    "#     value = row['IV_smooth']\n",
    "#     print(time_step_idx, width, height, value)\n",
    "#     # Assign the value to the corresponding position in the numpy array\n",
    "#     IV_array[time_step_idx, height, width] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(IV_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IV_array = IV_array.reshape((IV_array.shape[0], 5, 5, 1))\n",
    "# print(IV_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window_size = 21 # about one month\n",
    "# labels = IV_array[1:]\n",
    "# X = IV_array[:-1]\n",
    "\n",
    "dataset_train = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    data=IV_train[:-1],\n",
    "    targets=IV_train[window_size:],\n",
    "    sequence_length=window_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Add the last timepoints of the dataset to the validation set, for the computation of the\n",
    "# validation set performance is calculated within the window size too\n",
    "# So the validation set should start from the end of the training set\n",
    "\n",
    "IV_val_input = np.concatenate((IV_train[-window_size:], IV_val), axis=0)\n",
    "\n",
    "dataset_val = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    data=IV_val_input[:-1],\n",
    "    targets=IV_val_input[window_size:],\n",
    "    sequence_length=window_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "IV_test_input = np.concatenate((IV_val[-window_size:], IV_test), axis=0)\n",
    "\n",
    "dataset_test= tf.keras.utils.timeseries_dataset_from_array(\n",
    "    data=IV_test_input[:-1],\n",
    "    targets=IV_test_input[window_size:],\n",
    "    sequence_length=window_size,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model(n_params, \n",
    "#                  dropout, \n",
    "#                  recurrent_dropout, \n",
    "#                  n_convlstm_layers = 2,\n",
    "#                  hidden_activation =  tf.keras.activations.tanh, \n",
    "#                  optimizer = keras.optimizers.Adam()):\n",
    "\n",
    "#     # input layer\n",
    "#     input_layer = layers.Input(shape= (None,5,5,1) )\n",
    "    \n",
    "#     # lstm layers\n",
    "#     lstm = input_layer\n",
    "#     for i in range( n_convlstm_layers ):\n",
    "#         lstm =  layers.ConvLSTM2D( \n",
    "#             kernel_size= (1,1), \n",
    "#             filters=n_params, \n",
    "#             data_format= 'channels_last', \n",
    "#             return_sequences = i<n_convlstm_layers-1,\n",
    "#             activation=hidden_activation,\n",
    "#             padding = \"same\",\n",
    "#             dropout=dropout, \n",
    "#             recurrent_dropout=recurrent_dropout\n",
    "#         )( lstm )\n",
    "#         lstm = layers.BatchNormalization()(lstm)    \n",
    "\n",
    "#     output = layers.Conv2D(\n",
    "#         filters=1, kernel_size=(1, 1), activation=\"linear\", padding=\"same\"\n",
    "#     )( lstm )\n",
    "#     output_layer = layers.Reshape((5,5))(output)\n",
    "\n",
    "#     # compile\n",
    "#     model = models.Model( input_layer, output_layer )\n",
    "#     model.compile(\n",
    "#         loss= \"MAE\",\n",
    "#         optimizer=optimizer, \n",
    "#     ) \n",
    "    \n",
    "#     print(model.summary())\n",
    "#     return model\n",
    "# model = create_model(n_params=10,dropout=0.1,recurrent_dropout=0.1,n_convlstm_layers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(model, \n",
    "#                 x_train, \n",
    "#                 y_train,\n",
    "#                 verbose = True, \n",
    "#                 save : \"dir\" = False,\n",
    "#                 training_kwarg_overwrites : \"dict\" = {} ):\n",
    "    \n",
    "#     # train until we run out of improvement\n",
    "#     callbacks = [\n",
    "#         keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),\n",
    "#         keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=15),\n",
    "#     ]\n",
    "    \n",
    "#     # train model\n",
    "#     training_kwargs = {\n",
    "#         \"x\" : x_train,\n",
    "#         \"y\" : y_train, #dataset[\"train\"][\"y_scaled\"],\n",
    "#         \"epochs\" : 200,\n",
    "#         \"batch_size\" : 64,\n",
    "#         \"verbose\" : verbose,\n",
    "#         \"validation_split\" : 0.2,\n",
    "#         \"callbacks\" : callbacks,\n",
    "#     } \n",
    "#     training_kwargs.update(training_kwarg_overwrites)\n",
    "#     train_hist = model.fit( **training_kwargs )\n",
    "    \n",
    "    \n",
    "#     if save:\n",
    "#         Path(save).mkdir(parents=True, exist_ok=True) # make a home for the models\n",
    "#         train_start, train_end = [ f( dataset[\"dates\"][\"train\"] ) for f in (min,max) ]\n",
    "#         model_name = \"-\".join( date.strftime(\"%Y%m%d\") for date in [train_start, train_end] )\n",
    "#         model.save( save+model_name )\n",
    "        \n",
    "#     return model, train_hist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import ConvLSTM2D, BatchNormalization, Flatten, Dense\n",
    "\n",
    "# time_steps = window_size\n",
    "# height = 5\n",
    "# width = 5\n",
    "# channels = 1\n",
    "\n",
    "# # Model definition\n",
    "# model = Sequential([\n",
    "#     ConvLSTM2D(filters=64, kernel_size=(3,3), activation='relu', \n",
    "#                return_sequences=True, input_shape=(time_steps, height, width, channels)),\n",
    "#     BatchNormalization(),\n",
    "#     ConvLSTM2D(filters=32, kernel_size=(3,3), activation='relu', return_sequences=False),\n",
    "#     Flatten(),\n",
    "#     Dense(128, activation='relu'),\n",
    "#     Dense(1)  # Predicting IV at a future time\n",
    "# ])\n",
    "\n",
    "# # Compile model\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv_lstm2d_2 (ConvLSTM2D)  (None, 21, 5, 260, 64)    150016    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 21, 5, 260, 64)   256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv_lstm2d_3 (ConvLSTM2D)  (None, 5, 260, 64)        295168    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 5, 260, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 5, 260, 1)         65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 445,761\n",
      "Trainable params: 445,505\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "TIME_STEPS = window_size\n",
    "HEIGHT = len(data_train['moneyness_enc'].unique())\n",
    "WIDTH = len(data_train['maturity'].unique())\n",
    "CHANNELS = 1\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# ConvLSTM2D expects 5D input: (batch, time, height, width, channels)\n",
    "model.add(ConvLSTM2D(filters=64, kernel_size=(3, 3),\n",
    "                     padding='same', return_sequences=True,\n",
    "                     input_shape=(TIME_STEPS, HEIGHT, WIDTH, CHANNELS)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(ConvLSTM2D(filters=64, kernel_size=(3, 3),\n",
    "                     padding='same', return_sequences=False))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Final 3D convolution to map to the next frame\n",
    "model.add(tf.keras.layers.Conv2D(filters=1, kernel_size=(1, 1),\n",
    "                                 activation='sigmoid', padding='same'))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon)\n",
    "model.compile(loss='mse', optimizer=optimizer)\n",
    "\n",
    "# Double check the architecture, and the activaiton function\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "75/75 [==============================] - 20s 237ms/step - loss: 0.0721 - val_loss: 0.0441\n",
      "Epoch 2/100\n",
      "75/75 [==============================] - 17s 232ms/step - loss: 0.1921 - val_loss: 0.0208\n",
      "Epoch 3/100\n",
      "75/75 [==============================] - 18s 237ms/step - loss: 0.0315 - val_loss: 0.0150\n",
      "Epoch 4/100\n",
      "75/75 [==============================] - 18s 234ms/step - loss: 0.0129 - val_loss: 0.0036\n",
      "Epoch 5/100\n",
      "75/75 [==============================] - 17s 231ms/step - loss: 0.0041 - val_loss: 0.0028\n",
      "Epoch 6/100\n",
      "75/75 [==============================] - 18s 234ms/step - loss: 0.0025 - val_loss: 0.0028\n",
      "Epoch 7/100\n",
      "75/75 [==============================] - 18s 236ms/step - loss: 0.0020 - val_loss: 0.0025\n",
      "Epoch 8/100\n",
      "75/75 [==============================] - 18s 237ms/step - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 9/100\n",
      "75/75 [==============================] - 18s 234ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 10/100\n",
      "75/75 [==============================] - 17s 233ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 11/100\n",
      "75/75 [==============================] - 17s 232ms/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 12/100\n",
      "75/75 [==============================] - 17s 231ms/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Epoch 13/100\n",
      "75/75 [==============================] - 18s 235ms/step - loss: 0.0013 - val_loss: 9.9609e-04\n",
      "Epoch 14/100\n",
      "75/75 [==============================] - 18s 236ms/step - loss: 0.0013 - val_loss: 9.5215e-04\n",
      "Epoch 15/100\n",
      "75/75 [==============================] - 18s 234ms/step - loss: 0.0012 - val_loss: 9.3062e-04\n",
      "Epoch 16/100\n",
      "75/75 [==============================] - 17s 233ms/step - loss: 0.0012 - val_loss: 9.0679e-04\n",
      "Epoch 17/100\n",
      "75/75 [==============================] - 17s 233ms/step - loss: 0.0012 - val_loss: 8.8516e-04\n",
      "Epoch 18/100\n",
      "75/75 [==============================] - 17s 231ms/step - loss: 0.0012 - val_loss: 8.7398e-04\n",
      "Epoch 19/100\n",
      "75/75 [==============================] - 18s 235ms/step - loss: 0.0011 - val_loss: 8.5841e-04\n",
      "Epoch 20/100\n",
      "75/75 [==============================] - 18s 237ms/step - loss: 0.0011 - val_loss: 8.4968e-04\n",
      "Epoch 21/100\n",
      "75/75 [==============================] - 18s 237ms/step - loss: 0.0011 - val_loss: 8.4110e-04\n",
      "Epoch 22/100\n",
      "75/75 [==============================] - 18s 237ms/step - loss: 0.0011 - val_loss: 8.3582e-04\n",
      "Epoch 23/100\n",
      "75/75 [==============================] - 18s 236ms/step - loss: 0.0011 - val_loss: 8.2880e-04\n",
      "Epoch 24/100\n",
      "75/75 [==============================] - 18s 236ms/step - loss: 0.0011 - val_loss: 8.2274e-04\n",
      "Epoch 25/100\n",
      "75/75 [==============================] - 18s 237ms/step - loss: 0.0011 - val_loss: 8.2315e-04\n",
      "Epoch 26/100\n",
      "75/75 [==============================] - 18s 236ms/step - loss: 0.0011 - val_loss: 8.1210e-04\n",
      "Epoch 27/100\n",
      "75/75 [==============================] - 18s 234ms/step - loss: 0.0011 - val_loss: 8.1783e-04\n",
      "Epoch 28/100\n",
      "75/75 [==============================] - 18s 235ms/step - loss: 0.0011 - val_loss: 8.0536e-04\n",
      "Epoch 29/100\n",
      "75/75 [==============================] - 18s 234ms/step - loss: 0.0011 - val_loss: 8.0978e-04\n",
      "Epoch 30/100\n",
      "75/75 [==============================] - 18s 234ms/step - loss: 0.0011 - val_loss: 8.0424e-04\n",
      "Epoch 31/100\n",
      "75/75 [==============================] - 18s 234ms/step - loss: 0.0011 - val_loss: 7.9258e-04\n",
      "Epoch 32/100\n",
      "75/75 [==============================] - 18s 234ms/step - loss: 0.0011 - val_loss: 7.9571e-04\n",
      "Epoch 33/100\n",
      "75/75 [==============================] - 18s 234ms/step - loss: 0.0011 - val_loss: 7.8076e-04\n",
      "Epoch 34/100\n",
      "75/75 [==============================] - 18s 235ms/step - loss: 0.0011 - val_loss: 7.8632e-04\n",
      "Epoch 35/100\n",
      "75/75 [==============================] - 18s 234ms/step - loss: 0.0011 - val_loss: 7.7794e-04\n",
      "Epoch 36/100\n",
      "75/75 [==============================] - 18s 234ms/step - loss: 0.0011 - val_loss: 7.7976e-04\n",
      "Epoch 37/100\n",
      "75/75 [==============================] - 18s 236ms/step - loss: 0.0011 - val_loss: 7.7349e-04\n",
      "Epoch 38/100\n",
      "75/75 [==============================] - 18s 234ms/step - loss: 0.0011 - val_loss: 7.7900e-04\n",
      "Epoch 39/100\n",
      "75/75 [==============================] - 18s 234ms/step - loss: 0.0011 - val_loss: 7.7009e-04\n",
      "Epoch 40/100\n",
      "75/75 [==============================] - 18s 236ms/step - loss: 0.0010 - val_loss: 7.8044e-04\n",
      "Epoch 41/100\n",
      "75/75 [==============================] - 18s 239ms/step - loss: 0.0010 - val_loss: 7.6501e-04\n",
      "Epoch 42/100\n",
      "75/75 [==============================] - 18s 234ms/step - loss: 0.0010 - val_loss: 7.7327e-04\n",
      "Epoch 43/100\n",
      "75/75 [==============================] - 18s 234ms/step - loss: 0.0010 - val_loss: 7.6148e-04\n",
      "Epoch 44/100\n",
      "75/75 [==============================] - 18s 238ms/step - loss: 0.0010 - val_loss: 7.6927e-04\n",
      "Epoch 45/100\n",
      "75/75 [==============================] - 18s 236ms/step - loss: 0.0010 - val_loss: 7.5519e-04\n",
      "Epoch 46/100\n",
      "75/75 [==============================] - 18s 236ms/step - loss: 0.0010 - val_loss: 7.6110e-04\n",
      "Epoch 47/100\n",
      "75/75 [==============================] - 18s 236ms/step - loss: 0.0010 - val_loss: 7.5124e-04\n",
      "Epoch 48/100\n",
      "75/75 [==============================] - 18s 237ms/step - loss: 0.0010 - val_loss: 7.5478e-04\n",
      "Epoch 49/100\n",
      "75/75 [==============================] - 18s 236ms/step - loss: 0.0010 - val_loss: 7.5082e-04\n",
      "Epoch 50/100\n",
      "75/75 [==============================] - 18s 237ms/step - loss: 0.0010 - val_loss: 7.5371e-04\n",
      "Epoch 51/100\n",
      "75/75 [==============================] - 18s 236ms/step - loss: 9.9783e-04 - val_loss: 7.4821e-04\n",
      "Epoch 52/100\n",
      "75/75 [==============================] - 18s 236ms/step - loss: 0.0010 - val_loss: 7.4680e-04\n",
      "Epoch 53/100\n",
      "75/75 [==============================] - 18s 237ms/step - loss: 9.9018e-04 - val_loss: 7.4784e-04\n",
      "Epoch 54/100\n",
      "75/75 [==============================] - 18s 236ms/step - loss: 9.9350e-04 - val_loss: 7.4829e-04\n",
      "Epoch 55/100\n",
      "75/75 [==============================] - 18s 236ms/step - loss: 9.8638e-04 - val_loss: 7.4678e-04\n",
      "Epoch 56/100\n",
      "75/75 [==============================] - 18s 235ms/step - loss: 9.9001e-04 - val_loss: 7.4350e-04\n",
      "Epoch 57/100\n",
      "75/75 [==============================] - 18s 234ms/step - loss: 9.9048e-04 - val_loss: 7.4817e-04\n",
      "Epoch 58/100\n",
      "75/75 [==============================] - 17s 233ms/step - loss: 9.9165e-04 - val_loss: 7.4184e-04\n",
      "Epoch 59/100\n",
      "75/75 [==============================] - 18s 238ms/step - loss: 9.9809e-04 - val_loss: 7.4942e-04\n",
      "Epoch 60/100\n",
      "75/75 [==============================] - 18s 238ms/step - loss: 9.8964e-04 - val_loss: 7.4379e-04\n",
      "Epoch 61/100\n",
      "75/75 [==============================] - 18s 238ms/step - loss: 9.7525e-04 - val_loss: 7.5427e-04\n",
      "Epoch 62/100\n",
      "75/75 [==============================] - 18s 234ms/step - loss: 9.7198e-04 - val_loss: 7.4381e-04\n",
      "Epoch 63/100\n",
      "75/75 [==============================] - 17s 232ms/step - loss: 9.7300e-04 - val_loss: 7.5375e-04\n",
      "Epoch 64/100\n",
      "75/75 [==============================] - 18s 237ms/step - loss: 9.7329e-04 - val_loss: 7.3413e-04\n",
      "Epoch 65/100\n",
      "75/75 [==============================] - 18s 236ms/step - loss: 9.6952e-04 - val_loss: 7.4802e-04\n",
      "Epoch 66/100\n",
      "75/75 [==============================] - 17s 232ms/step - loss: 9.6570e-04 - val_loss: 7.3282e-04\n",
      "Epoch 67/100\n",
      "75/75 [==============================] - 17s 233ms/step - loss: 9.6419e-04 - val_loss: 7.5372e-04\n",
      "Epoch 68/100\n",
      "75/75 [==============================] - 18s 235ms/step - loss: 9.6170e-04 - val_loss: 7.3419e-04\n",
      "Epoch 69/100\n",
      "75/75 [==============================] - 18s 235ms/step - loss: 9.5813e-04 - val_loss: 7.4413e-04\n",
      "Epoch 70/100\n",
      "75/75 [==============================] - 18s 235ms/step - loss: 9.6266e-04 - val_loss: 7.3890e-04\n",
      "Epoch 71/100\n",
      "75/75 [==============================] - 17s 233ms/step - loss: 9.5056e-04 - val_loss: 7.4560e-04\n",
      "Epoch 72/100\n",
      "75/75 [==============================] - 17s 233ms/step - loss: 9.5527e-04 - val_loss: 7.5894e-04\n",
      "Epoch 73/100\n",
      "75/75 [==============================] - 17s 232ms/step - loss: 9.5066e-04 - val_loss: 7.3893e-04\n",
      "Epoch 74/100\n",
      "75/75 [==============================] - 18s 234ms/step - loss: 9.5820e-04 - val_loss: 7.6567e-04\n",
      "Epoch 75/100\n",
      "75/75 [==============================] - 18s 239ms/step - loss: 9.6405e-04 - val_loss: 7.3028e-04\n",
      "Epoch 76/100\n",
      "75/75 [==============================] - 17s 233ms/step - loss: 9.5748e-04 - val_loss: 7.4198e-04\n",
      "Epoch 77/100\n",
      "75/75 [==============================] - 18s 234ms/step - loss: 9.6290e-04 - val_loss: 7.3060e-04\n",
      "Epoch 78/100\n",
      "75/75 [==============================] - 17s 233ms/step - loss: 9.6059e-04 - val_loss: 7.3129e-04\n",
      "Epoch 79/100\n",
      "75/75 [==============================] - 17s 232ms/step - loss: 9.6641e-04 - val_loss: 7.3519e-04\n",
      "Epoch 80/100\n",
      "75/75 [==============================] - 17s 231ms/step - loss: 9.5598e-04 - val_loss: 7.3737e-04\n",
      "Epoch 81/100\n",
      "75/75 [==============================] - 18s 237ms/step - loss: 9.4887e-04 - val_loss: 7.3916e-04\n",
      "Epoch 82/100\n",
      "75/75 [==============================] - 18s 238ms/step - loss: 9.3546e-04 - val_loss: 7.5567e-04\n",
      "Epoch 83/100\n",
      "75/75 [==============================] - 17s 233ms/step - loss: 9.3831e-04 - val_loss: 7.3590e-04\n",
      "Epoch 84/100\n",
      "75/75 [==============================] - 18s 234ms/step - loss: 9.2968e-04 - val_loss: 7.4148e-04\n",
      "Epoch 85/100\n",
      "75/75 [==============================] - 18s 237ms/step - loss: 9.3676e-04 - val_loss: 7.2730e-04\n",
      "Epoch 86/100\n",
      "75/75 [==============================] - 17s 233ms/step - loss: 9.3753e-04 - val_loss: 7.3620e-04\n",
      "Epoch 87/100\n",
      "75/75 [==============================] - 17s 231ms/step - loss: 9.3432e-04 - val_loss: 7.3464e-04\n",
      "Epoch 88/100\n",
      "75/75 [==============================] - 18s 235ms/step - loss: 9.2676e-04 - val_loss: 7.3756e-04\n",
      "Epoch 89/100\n",
      "75/75 [==============================] - 18s 235ms/step - loss: 9.2348e-04 - val_loss: 7.3622e-04\n",
      "Epoch 90/100\n",
      "75/75 [==============================] - 18s 235ms/step - loss: 9.1782e-04 - val_loss: 7.3642e-04\n",
      "Epoch 91/100\n",
      "75/75 [==============================] - 18s 235ms/step - loss: 9.1863e-04 - val_loss: 7.2980e-04\n",
      "Epoch 92/100\n",
      "75/75 [==============================] - 17s 232ms/step - loss: 9.2219e-04 - val_loss: 7.3979e-04\n",
      "Epoch 93/100\n",
      "75/75 [==============================] - 17s 232ms/step - loss: 9.2006e-04 - val_loss: 7.2057e-04\n",
      "Epoch 94/100\n",
      "75/75 [==============================] - 17s 232ms/step - loss: 9.1669e-04 - val_loss: 7.1973e-04\n",
      "Epoch 95/100\n",
      "75/75 [==============================] - 18s 237ms/step - loss: 9.1040e-04 - val_loss: 7.2759e-04\n",
      "Epoch 96/100\n",
      "75/75 [==============================] - 18s 235ms/step - loss: 9.1501e-04 - val_loss: 7.2026e-04\n",
      "Epoch 97/100\n",
      "75/75 [==============================] - 17s 233ms/step - loss: 9.1619e-04 - val_loss: 7.3022e-04\n",
      "Epoch 98/100\n",
      "75/75 [==============================] - 17s 233ms/step - loss: 9.0917e-04 - val_loss: 7.1401e-04\n",
      "Epoch 99/100\n",
      "75/75 [==============================] - 17s 232ms/step - loss: 9.0265e-04 - val_loss: 7.1633e-04\n",
      "Epoch 100/100\n",
      "75/75 [==============================] - 17s 231ms/step - loss: 9.0030e-04 - val_loss: 7.1521e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x157c19f00d0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset_train, epochs=epochs, validation_data=dataset_val, callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                    patience=patience,\n",
    "                                                    mode='min')])\n",
    "\n",
    "# Takes about 40 min on dell xps laptop, 6.5 min on PC for short term\n",
    "# 24 min on PC for long term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 69ms/step\n",
      "8/8 [==============================] - 1s 71ms/step\n"
     ]
    }
   ],
   "source": [
    "# Next step; functions for the IVRMSE and R_oos!!!!!\n",
    "# H-step ahead performance!!!!!!\n",
    "# WRITE IT ALL TO RESULTS!!!!!!\n",
    "# Lower learning rate, beneficial, or leave it? Probably leave it\n",
    "# All the HYPERPARAMETERS!!!!!! kernel strides, window sizes, parameters, layers, ALL OF THEM\n",
    "# COVARIATES!!!!!!, Do all the hyperparameters again, and then only save the TEST PERFORMANCE\n",
    "# MODEL ARCHITECTURE!!!!\n",
    "# Investigate what happens, if you leave it as 0... We cannot do the interpolation properly, to be frank.\n",
    "\n",
    "#THEN LONG TERM !!!!!!\n",
    "# Transformer models!!!!!!!!!!!!!!!!\n",
    "pred_val = model.predict(dataset_val)\n",
    "pred_test = model.predict(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it in the compile, but also call it afterward\n",
    "# double check formula and also make it for R_oos\n",
    "# Should also work per h-step ahead \n",
    "def calculate_ivrmse(y_true, y_pred, all_points=False):\n",
    "    if not all_points:\n",
    "        ivrmse = tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "    else:\n",
    "        sq_error = tf.square(y_true - y_pred)\n",
    "        error_surface = tf.reduce_mean(sq_error, axis=[1 , 2])\n",
    "        ivrmse = tf.sqrt(error_surface)\n",
    "\n",
    "    return ivrmse.numpy()\n",
    "\n",
    "# model.compile(optimizer='adam', loss='mse', metrics=[ivrmse_metric])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.026743365\n",
      "0.026743365\n"
     ]
    }
   ],
   "source": [
    "print(calculate_ivrmse(IV_val, pred_val))\n",
    "print(calculate_ivrmse(IV_val, pred_val, all_points=False))\n",
    "# The loss is less, than in the optimization.. probably a different metric, or formula than mse\n",
    "# check values of the papers that are like yours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6964496\n"
     ]
    }
   ],
   "source": [
    "def calculate_r_oos(y_true, y_pred, all_points=False):\n",
    "    if not all_points:\n",
    "        ss_res = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "        mean_IV = tf.reduce_mean(y_true, axis=[1, 2], keepdims=True) # should be shape of 114 long\n",
    "        # print(mean_IV[0], mean_IV[1], mean_IV[2])\n",
    "        # print(y_true[0], y_true[1], y_true[2])\n",
    "        ss_tot = tf.reduce_sum(tf.square(y_true - mean_IV))\n",
    "        # print((y_true - mean_IV)[0])\n",
    "        r2 = 1 - ss_res/ss_tot\n",
    "    else:\n",
    "        ss_res = tf.reduce_sum(tf.square(y_true - y_pred), axis=[1, 2])\n",
    "        mean_IV = tf.reduce_mean(y_true, axis=[1, 2], keepdims=True)\n",
    "        ss_tot = tf.reduce_sum(tf.square(y_true - mean_IV), axis=[1, 2])\n",
    "        r2 = 1 - ss_res/ss_tot\n",
    "    return r2.numpy()\n",
    "\n",
    "print(calculate_r_oos(IV_val, pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model again on BOTH train and validation, and then investigate the TEST PERFORMANCE!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05695225\n",
      "0.67053\n"
     ]
    }
   ],
   "source": [
    "print(calculate_ivrmse(IV_test, pred_test))\n",
    "print(calculate_r_oos(IV_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(y_real, y_pred):\n",
    "    ivrmse = calculate_ivrmse(y_real, y_pred)\n",
    "    ivrmse_h = calculate_ivrmse(y_real, y_pred, all_points=True)\n",
    "    r_oos = calculate_r_oos(y_real, y_pred)\n",
    "    r_oos_h = calculate_r_oos(y_real, y_pred, all_points=True)\n",
    "\n",
    "    return ivrmse, ivrmse_h, r_oos, r_oos_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(folder_path, ivrmse, r_oos, ivrmse_h, r_oos_h, surface, surface_pred):\n",
    "\n",
    "    ivrmse_path = folder_path / Path(\"ivrmse\")\n",
    "    r_oos_path = folder_path / Path(\"r_oos\")\n",
    "    ivrmse_h_path = folder_path / Path(\"ivrmse_h\")\n",
    "    r_oos_h_path = folder_path / Path(\"r_oos_h\")\n",
    "    surface_path = folder_path / Path(\"surface\")\n",
    "    surface_pred_path = folder_path / Path(\"surface_pred\")\n",
    "\n",
    "    if not ivrmse_path.exists():\n",
    "        ivrmse_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if not r_oos_path.exists():\n",
    "        r_oos_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if not ivrmse_h_path.exists():\n",
    "        ivrmse_h_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if not r_oos_h_path.exists():\n",
    "        r_oos_h_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if not surface_path.exists():\n",
    "        surface_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if not surface_pred_path.exists():\n",
    "        surface_pred_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    np.save(ivrmse_path / f\"{window_size}_{h_step}.npy\", ivrmse)\n",
    "    np.save(r_oos_path / f\"{window_size}_{h_step}.npy\", r_oos)\n",
    "    np.save(ivrmse_h_path / f\"{window_size}_{h_step}.npy\", ivrmse_h)\n",
    "    np.save(r_oos_h_path / f\"{window_size}_{h_step}.npy\", r_oos_h)\n",
    "    np.save(surface_path/ f\"{window_size}_{h_step}.npy\", surface)\n",
    "    np.save(surface_pred_path / f\"{window_size}_{h_step}.npy\", surface_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = Path(f\"results/test_{run}\")\n",
    "ivrmse, ivrmse_h, r_oos, r_oos_h = get_results(IV_test, pred_test)\n",
    "write_results(folder_path, ivrmse, r_oos, ivrmse_h, r_oos_h, IV_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.2536705 ]\n",
      " [ 0.8199241 ]\n",
      " [ 0.800799  ]\n",
      " [ 0.85355824]\n",
      " [ 0.804582  ]\n",
      " [ 0.7611677 ]\n",
      " [ 0.84622806]\n",
      " [ 0.7885027 ]\n",
      " [ 0.8726183 ]\n",
      " [ 0.83219767]\n",
      " [ 0.8029318 ]\n",
      " [ 0.86571825]\n",
      " [ 0.77316964]\n",
      " [ 0.7527638 ]\n",
      " [ 0.76095337]\n",
      " [ 0.78598344]\n",
      " [ 0.8546557 ]\n",
      " [ 0.86272913]\n",
      " [ 0.7356323 ]\n",
      " [ 0.79146326]\n",
      " [ 0.8553258 ]\n",
      " [ 0.7752081 ]\n",
      " [ 0.64446795]\n",
      " [ 0.5794525 ]\n",
      " [ 0.81432474]\n",
      " [ 0.85043055]\n",
      " [ 0.84998024]\n",
      " [ 0.7686269 ]\n",
      " [ 0.79091096]\n",
      " [ 0.56344134]\n",
      " [ 0.82608384]\n",
      " [ 0.83274114]\n",
      " [ 0.73001575]\n",
      " [ 0.7314482 ]\n",
      " [ 0.82104015]\n",
      " [-0.29882216]\n",
      " [ 0.7059878 ]\n",
      " [ 0.7372205 ]\n",
      " [ 0.5900607 ]\n",
      " [ 0.7503256 ]\n",
      " [ 0.83801675]\n",
      " [ 0.6299157 ]\n",
      " [ 0.82373875]\n",
      " [ 0.6074254 ]\n",
      " [ 0.8452396 ]\n",
      " [ 0.75253874]\n",
      " [ 0.811813  ]\n",
      " [ 0.90128154]\n",
      " [ 0.74461067]\n",
      " [ 0.8762873 ]\n",
      " [ 0.69524664]\n",
      " [ 0.81041044]\n",
      " [ 0.74541485]\n",
      " [ 0.73105246]\n",
      " [ 0.8134964 ]\n",
      " [ 0.7025858 ]\n",
      " [ 0.6236203 ]\n",
      " [ 0.54182684]\n",
      " [ 0.84359205]\n",
      " [ 0.73678946]\n",
      " [ 0.70554686]\n",
      " [ 0.7227696 ]\n",
      " [ 0.7085861 ]\n",
      " [ 0.8282721 ]\n",
      " [ 0.7123053 ]\n",
      " [ 0.06441087]\n",
      " [ 0.6803988 ]\n",
      " [ 0.7296672 ]\n",
      " [ 0.688661  ]\n",
      " [ 0.6764977 ]\n",
      " [ 0.7153331 ]\n",
      " [ 0.5827189 ]\n",
      " [ 0.6987756 ]\n",
      " [ 0.8351903 ]\n",
      " [ 0.6167047 ]\n",
      " [ 0.8416707 ]\n",
      " [ 0.8051328 ]\n",
      " [ 0.7436335 ]\n",
      " [ 0.6785675 ]\n",
      " [ 0.02544391]\n",
      " [ 0.5896034 ]\n",
      " [ 0.7668958 ]\n",
      " [ 0.79422474]\n",
      " [ 0.58255064]\n",
      " [ 0.8102384 ]\n",
      " [ 0.84585226]\n",
      " [ 0.6181118 ]\n",
      " [ 0.7591103 ]\n",
      " [-0.00201058]\n",
      " [ 0.54856265]\n",
      " [ 0.7045326 ]\n",
      " [ 0.7774607 ]\n",
      " [ 0.58686763]\n",
      " [ 0.73108715]\n",
      " [ 0.6437947 ]\n",
      " [ 0.6918148 ]\n",
      " [ 0.66671133]\n",
      " [ 0.59245396]\n",
      " [ 0.5461378 ]\n",
      " [ 0.6389088 ]\n",
      " [ 0.6213372 ]\n",
      " [ 0.70714283]\n",
      " [ 0.5939911 ]\n",
      " [ 0.5986507 ]\n",
      " [ 0.7244234 ]\n",
      " [ 0.71962994]\n",
      " [ 0.7178762 ]\n",
      " [ 0.7357423 ]\n",
      " [ 0.6527608 ]\n",
      " [ 0.76626426]\n",
      " [ 0.5144477 ]\n",
      " [ 0.64744425]\n",
      " [ 0.69106615]\n",
      " [ 0.5456339 ]\n",
      " [ 0.54349124]\n",
      " [ 0.44913173]\n",
      " [ 0.7597823 ]\n",
      " [ 0.8326161 ]\n",
      " [ 0.76058805]\n",
      " [ 0.6034601 ]\n",
      " [ 0.5054266 ]\n",
      " [ 0.7005036 ]\n",
      " [ 0.67930806]\n",
      " [ 0.74984056]\n",
      " [ 0.62012136]\n",
      " [ 0.5556593 ]\n",
      " [ 0.7718351 ]\n",
      " [ 0.6489124 ]\n",
      " [ 0.7857399 ]\n",
      " [ 0.61299425]\n",
      " [ 0.63548684]\n",
      " [ 0.7576974 ]\n",
      " [-0.05972266]\n",
      " [ 0.57741594]\n",
      " [ 0.612676  ]\n",
      " [ 0.6389641 ]\n",
      " [ 0.79242283]\n",
      " [ 0.6034015 ]\n",
      " [ 0.6423098 ]\n",
      " [ 0.67208946]\n",
      " [ 0.6696723 ]\n",
      " [ 0.68015754]\n",
      " [ 0.68044084]\n",
      " [ 0.7286884 ]\n",
      " [ 0.47708178]\n",
      " [ 0.7429029 ]\n",
      " [ 0.7263374 ]\n",
      " [ 0.7053349 ]\n",
      " [ 0.7729121 ]\n",
      " [ 0.72163   ]\n",
      " [ 0.7943429 ]\n",
      " [ 0.61223817]\n",
      " [ 0.72625726]\n",
      " [ 0.53269005]\n",
      " [ 0.6641576 ]\n",
      " [ 0.77277607]\n",
      " [ 0.79572356]\n",
      " [ 0.7759352 ]\n",
      " [ 0.72035235]\n",
      " [ 0.7656809 ]\n",
      " [ 0.8093619 ]\n",
      " [ 0.6921732 ]\n",
      " [ 0.6456275 ]\n",
      " [ 0.7694103 ]\n",
      " [ 0.724016  ]\n",
      " [ 0.76340926]\n",
      " [ 0.6551074 ]\n",
      " [ 0.75604934]\n",
      " [ 0.66838753]\n",
      " [ 0.5926506 ]\n",
      " [ 0.6958413 ]\n",
      " [ 0.775292  ]\n",
      " [ 0.7398117 ]\n",
      " [ 0.66001594]\n",
      " [ 0.61332136]\n",
      " [ 0.7309831 ]\n",
      " [ 0.7464881 ]\n",
      " [ 0.70456785]\n",
      " [ 0.75859267]\n",
      " [ 0.7043174 ]\n",
      " [ 0.73103726]\n",
      " [ 0.6720948 ]\n",
      " [ 0.569847  ]\n",
      " [ 0.70326   ]\n",
      " [ 0.53780323]\n",
      " [ 0.6722171 ]\n",
      " [ 0.74077874]\n",
      " [ 0.7628884 ]\n",
      " [ 0.83169997]\n",
      " [-0.01429451]\n",
      " [ 0.6790692 ]\n",
      " [ 0.71401215]\n",
      " [ 0.56161714]\n",
      " [ 0.6529119 ]\n",
      " [ 0.793133  ]\n",
      " [ 0.6493649 ]\n",
      " [ 0.6327183 ]\n",
      " [ 0.7103863 ]\n",
      " [ 0.7255554 ]\n",
      " [ 0.8681839 ]\n",
      " [ 0.65881586]\n",
      " [ 0.76846194]\n",
      " [ 0.7115497 ]\n",
      " [ 0.7092585 ]\n",
      " [ 0.7523632 ]\n",
      " [ 0.7528624 ]\n",
      " [ 0.63809717]\n",
      " [ 0.65851176]\n",
      " [ 0.7405021 ]\n",
      " [ 0.73344445]\n",
      " [-0.0203476 ]\n",
      " [ 0.61636865]\n",
      " [ 0.7560423 ]\n",
      " [ 0.67044234]\n",
      " [-0.08350074]\n",
      " [ 0.5116327 ]\n",
      " [ 0.58259404]\n",
      " [ 0.671195  ]\n",
      " [ 0.72280455]\n",
      " [ 0.68716013]\n",
      " [ 0.7058198 ]\n",
      " [ 0.73842835]\n",
      " [ 0.7797448 ]\n",
      " [ 0.02412653]\n",
      " [ 0.56518954]\n",
      " [ 0.6022098 ]\n",
      " [ 0.6030053 ]\n",
      " [ 0.5278268 ]\n",
      " [ 0.7011703 ]\n",
      " [ 0.68086   ]\n",
      " [ 0.57263476]\n",
      " [ 0.73627174]\n",
      " [ 0.535611  ]\n",
      " [ 0.6316372 ]\n",
      " [ 0.5916052 ]\n",
      " [ 0.49044168]\n",
      " [ 0.6782379 ]\n",
      " [ 0.6046217 ]\n",
      " [ 0.6374552 ]\n",
      " [ 0.5931945 ]\n",
      " [ 0.6381638 ]\n",
      " [ 0.7334609 ]\n",
      " [ 0.6984383 ]\n",
      " [ 0.7250608 ]\n",
      " [ 0.6280596 ]\n",
      " [ 0.7050407 ]\n",
      " [ 0.7889144 ]\n",
      " [ 0.09682077]\n",
      " [ 0.7026347 ]\n",
      " [ 0.7100898 ]\n",
      " [ 0.75743085]\n",
      " [ 0.6941123 ]\n",
      " [ 0.6665567 ]]\n"
     ]
    }
   ],
   "source": [
    "print(r_oos_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = Path(f\"results/validation_{run}\")\n",
    "ivrmse, ivrmse_h, r_oos, r_oos_h = get_results(IV_val, pred_val)\n",
    "write_results(folder_path, ivrmse, r_oos, ivrmse_h, r_oos_h, IV_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the h-step ahead forecast, an autoregressive approach is used\n",
    "# Predict the one step ahead forecast (done now)\n",
    "# After, use this one step ahead forecast in the predict command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253, 5, 260, 1)\n"
     ]
    }
   ],
   "source": [
    "print(pred_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
